% !TEX root = ../thesis.tex
\section{Jazykové modelování}
\label{chap:asr:language}

Jazykový model (LM) je po parametrizaci a akustickém modelu další důležitou částí systému rozpoznávání řeči viz obr. \ref{fig:asr:decoding}. Jeho úkolem je poskytnout dekodéru co nejrychleji co nejpřesnější odhad apriorní pravděpodobnosti $P\left(W\right)$ pro libovolnou posloupnost slov $W$. Tuto pravděpodobnost je možné vyjádřit vztahem

\begin{equation}
  P\left(W\right) = \prod_{k=1}^{K} P\left(w_k | w_{k-1}\dots w_{1}\right),
  \label{eq:asr:language:W_full}
\end{equation}

\noindent kde $K$ je počet slov posloupnosti $W$. Pokud by byl proveden rozklad (\ref{eq:asr:language:W_full}), vyšlo by najevo, že pravděpodobnost výskytu slova $P\left(w_i\right),\ i \leq K$, je podmíněna pouze svou historií, tj. posloupností slov $w_1\ \dots\ w_{i-2}w_{i-1}$.

Systémy rozpoznávání řeči pracují obvykle s rozsáhlými slovníky, čítajícími stovky tisíc až jednotky milionů slov, proto nelze (obecně) předpokládat, že by bylo možné pravděpodobnosti v (\ref{eq:asr:language:W_full}) dostatečně robustně odhadnout pro libovolnou délku posloupnosti $K$.
Obvykle se proto provádí aproximace vztahu (\ref{eq:asr:language:W_full}), při níž dochází k redukci počtu odhadovaných parametrů. Nejčastějším postupem je stanovení ekvivalentních tříd slov na základě jejich slovní historie, tj. všechny historie $w_1\ \dots\ w_{i-2}w_{i-1}$, které se shodují v posledních $n-1$ slovech, jsou zařazeny do stejné třídy. Uvedené modely se nazývají \textbf{n-gramové modely}, přičemž \textit{n}-gramem se rozumí posloupnost $n$ za sebou jdoucích slov v pozorování jejich náhodného výběru, např. trénovacího korpusu obsahujícího textová data. Modely s $n=0$ se nazývají \textbf{zerogramy}, $n=1$ pak \textbf{unigramy}. Nejpoužívanější jsou pak \textbf{bigramy} ($n=2$) a \textbf{trigramy} ($n=3$). Pravděpodobnost $P\left(W\right)$ u $n$-gramového modelu lze určit pomocí vztahu

\begin{equation}
  P\left(W\right) = \prod_{k=1}^{K} P\left(w_k | w_{k-1}\dots w_{k-n+1}\right).
  \label{eq:asr:language:W}
\end{equation}

\noindent V ideálním případě by optimální model měl mít $n > 3$, ale v praxi se příliš často nevyužívají, protože s rostoucím řádem modelu enormně roste potřebná velikost množiny trénovacích dat. Například pro slovník s $N$ položkami existuje stále $N^{n}$ $n$-gramových statistik, které je potřeba odhadnout. Jak bylo zmíněno, odhad těchto statistik se provádí na základě relativních četností v trénovacích datech. Například u bigramů ($n=2$) a slovníku o velikosti $N=10^{5}$ je zapotřebí odhadnout $10^{10}$ různých bigramů a k tomu je zapotřebí relativně velké trénovací množiny. Je zřejmé, že většina z těchto $10^{10}$ bigramů se vůbec neobjeví v balíku trénovacích dat. Těmto \uv{neviděným} bigramům tedy odpovídá nulová pravděpodobnost, což dle vztahu (\ref{eq:asr:language:W}) vyústí v nulovou pravděpodobnost $P\left(W\right)$. K řešení tohoto problému se využívají techniky \uv{vyhlazování}. Jejich úkolem je odhad pravděpodobností těchto neviděných jevů s využitím tzv. ústupových, interpolačních a diskontních schémat \cite{Psutka2006}.

Výstupem akustického modelu jsou většinou fonémy ve zvolené fonetické abecedě (např. SAMPA). Nezbytnou součástí systémů rozpoznávání řeči je výslovnostní slovník, který obsahuje kombinace slov a fonetického přepisu těchto slov.
%Z principu může mít jedno slovo více fonetických transkripcí.
Tento slovník umožňuje výpočet pravděpodobnosti $P\left(W\right)$ na základě výstupu akustického modelu.
