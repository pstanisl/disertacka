% !TEX root = ../thesis.tex
\section{Jazykové modelování}
\label{chap:asr:language}

Jazykový model (obr. \ref{fig:asr:decoding}) je po parametrizaci a akustickém modelu další důležitou částí systému rozpoznávání řeči. Jeho úkolem je poskytnout dekodéru co nejrychleji nejpřesnější odhad apriorní pravděpodobnosti $P\left(W\right)$ pro libovolnou posloupnost slov $W$. Tuto pravděpodonost je možné vyjádřit vztahem

\begin{equation}
  P\left(W\right) = \prod_{k=1}^{K} P\left(w_k | w_{k-1}\dots w_{1}\right),
  \label{eq:asr:language:W_full}
\end{equation}

\noindent kde $K$ počet slov posloupnosti $W$. Pokud by byl proveden rozklad (\ref{eq:asr:language:W_full}) vyšlo by najevo, že pravděpodobnost výskytu slova $P\left(w_i\right),\ i \leq K$ je podmíněna pouze svou historií, tj. posloupností slov $w_1\ \dots\ w_{i-2}w_{i-1}$.

Systémy rozpoznávání řeči pracují obvykle s rozsáhlými slovníky, čítající stovky tisíc až jednotky milionů slov, není možné předpokládat, že by bylo možné pravděpodobnosti v (\ref{eq:asr:language:W_full}) dostatečně robustně odhadnout pro libovolnou délku posloupnosti $K$.

Obvykle se proto provádí aproximace vztahu (\ref{eq:asr:language:W_full}), při níchž docházi k redukci počtu odhadovaných parametrů. Nejčastějším způsobem je stanovení ekvivalentních tříd slov na základě jejich slovní historie, tj. všechny historie $w_1\ \dots\ w_{i-2}w_{i-1}$, které se shodují v posledních $n-1$ slovech, jsou zařazeny do stejné třídy. Uvedené modely se nazývají \textbf{n-gramové modely}. Přitom \textit{n}-gramem se rozumí posloupnost $n$ za sebou jdoucích slov v pozorování jejich náhodného výběru, např. trénovacího korpusu obsahujícího textová data. Modely s $n=0$ se nazývají \textbf{zerogramy}, $n=1$ pak \textbf{unigramy}. Nejpoužívanější jsou pak \textbf{bigramy} ($n=2$) a \textbf{trigramy} ($n=3$). Pravděpodobnost $P\left(W\right)$ u $n$-gramového modelu se vypočte vztahem

\begin{equation}
  P\left(W\right) = \prod_{k=1}^{K} P\left(w_k | w_{k-1}\dots w_{k-n+1}\right).
  \label{eq:asr:language:W}
\end{equation}

\noindent V ideálním případě by optimální model měl mít $n > 3$, ale v praxi se tyto modely moc nepoužívají, protože s rostoucím řádem modelu enormně roste potřebná velikost trénovacích dat. Například pro slovník s $N$ položkami existuje stále $N^{n}$ $n$-gramových statistik, které je potřeba odhadnout. Jak bylo zmíněno odhad těchto statistik se provádí na základě relativních četností v trénovacích datech. Například u bigramů ($n=2$) a slovníku o velikosti $N=10^{5}$ je zapotřebí odhadnout $10^{10}$ různých bigramů a k tomu je zapotřebí relativně velké trénovací množiny. Je zřejmé, že většina z těchto $10^{10}$ bigramů se vůbec neobjeví v datech. Těmto \uv{neviděným} bigramům tedy odpovídá nulová pravděpodobnost, což vyústí v nulovou pravděpodobnost $P\left(W\right)$ (\ref{eq:asr:language:W}). K řešení tohoto problému se používá technik \uv{vyhlazování}. Jejich cílem je odhad pravděpodobností těchto neviděných jevů s využitím tzv. ústupových, interpolačních a diskontních schémat \cite{Psutka2006}.

Výstupem akustického modelu jsou většinou fonémy ve zvolené fonetické abecedě (např. SAMPA). Nezbytnou součástí systémů rozpoznávání řeči, tak je výslovnostní slovník, který obsahuje kombinace slov a fonetického přepisu těchto slov.
%Z principu může mít jedno slovo více fonetických transkripcí.
Tento slovník umožňuje výpočet $P\left(W\right)$ na základě výstupu akustického modelu.
